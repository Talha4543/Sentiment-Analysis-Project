# -*- coding: utf-8 -*-
"""app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pL9ieINr3IIfTYAlAGqSOwtBCffnM6Gm
"""

!pip install streamlit
import streamlit as st
import pandas as pd
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Activation, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
import nltk
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('wordnet')

# Preprocessing functions
STOPWORDS = set(stopwords.words('english'))
lemmatizer = nltk.WordNetLemmatizer()

def preprocess_text(text):
    # Remove URLs, mentions, and numbers
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'@\w+', '', text)
    text = re.sub(r'\d+', '', text)

    # Remove punctuations
    text = re.sub(r'[^\w\s]', '', text)

    # Remove stopwords
    text = " ".join([word for word in text.split() if word not in STOPWORDS])

    # Lemmatize words
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

# Load dataset with a limit of 20,000 samples
@st.cache
def load_data():
    data = pd.read_csv("/content/training.1600000.processed.noemoticon.csv", encoding="ISO-8859-1", engine="python")
    data.columns = ["label", "time", "date", "query", "username", "text"]
    data = data[["label", "text"]]
    data['label'] = data['label'].apply(lambda x: 1 if x == 4 else 0)  # Convert labels: 4 -> Positive, 0 -> Negative
    return data.sample(n=20000, random_state=42)  # Select 20,000 random samples

# Build TensorFlow model
def tensorflow_based_model(max_len):
    inputs = Input(name='inputs', shape=[max_len])
    layer = Embedding(2000, 50, input_length=max_len)(inputs)
    layer = LSTM(64)(layer)
    layer = Dense(256, name='FC1')(layer)
    layer = Activation('relu')(layer)
    layer = Dropout(0.5)(layer)
    layer = Dense(1, name='out_layer')(layer)
    layer = Activation('sigmoid')(layer)
    model = Model(inputs=inputs, outputs=layer)
    return model

# Load and preprocess data
data = load_data()
data['text'] = data['text'].apply(preprocess_text)

# Tokenization
tokenizer = Tokenizer(num_words=2000)
tokenizer.fit_on_texts(data['text'])
max_len = 50
X = tokenizer.texts_to_sequences(data['text'])
X = pad_sequences(X, maxlen=max_len)
Y = data['label']

# Train-test split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Train model
model = tensorflow_based_model(max_len)
model.compile(loss='binary_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=80, epochs=1, validation_split=0.1)

# Streamlit Application
st.title("Sentiment Analysis")

st.write("Enter a tweet or text to predict the sentiment.")

# Input text box
input_text = st.text_area("Input Text", height=150)

if st.button("Predict Sentiment"):
    if input_text.strip():
        preprocessed_text = preprocess_text(input_text)
        input_sequence = tokenizer.texts_to_sequences([preprocessed_text])
        input_padded = pad_sequences(input_sequence, maxlen=max_len)
        prediction = model.predict(input_padded)
        sentiment = "Positive" if prediction[0][0] > 0.5 else "Negative"
        st.success(f"Sentiment: {sentiment}")
    else:
        st.error("Please enter some text!")